## 第一条

文本序列标注问题是什么？列举三种以上的文本序列标注问题（10分）

序列标注就是对于一个一维线性输入序列，给线性序列中的每个元素打上标签集合中的某个标签。所以，其本质上是对线性序列中每个元素根据上下文内容进行分类的问题。文本序列标注问题就是对文本序列的每一个词打上标签，常见的文本序列标注问题有分词，词性标注，命名实体识别(实体抽取)，关键词抽取，词义角色标注等。

- 中文分词（Chinese Word Segmentation）：将给定句子切分为具有合理语义的词序列。在分词问题中，序列节点的“词”对应为句子中的每个字，节点的标签空间为{B,I,E,S}。B表示这个字是某个词的开头，I表示这个字是某个词的中间部分，E表示这个字是某个词的结尾，S表示这个字单独成词。每个字最终都会打上对应标签，最终根据标签序列来确定分词结果。
- 词性标注（Part-of-Speech Tagging）：给定已分词的句子，将句子中的所有词标记词性。这里的“词”对应的就是已分词的词序列中的词，节点的标签空间为词性标记空间如{noun,verb,adj,…}。每个词最终都会打上词性标签。
- 命名实体识别（Named Entity Recognition）：找出给定句子中的命名实体（常见的有人名、地名、机构名）。NER问题中，序列节点的“词”对应为句子中的每个字，节点的标签空间为{B,I,E,O}。B表示这个字是某个命名实体的开头，I表示这个字是某个命名实体的中间部分，E表示这个字是某个命名实体的结尾，O表示这个字不属于命名实体部分。根据最后的标签序列确定识别结果。



## 第二条

请阐述词性标注问题的特点和挑战，并给出一个该任务的解决方案，简述该方案的思想。

特点：
词性标注就是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程。

挑战：
1. 常用词兼类现象严重。《现代汉语八百词》收取的常用词中，兼类词所占的比例高达22.5%，而且发现越是常用的词，不同的用法越多。由于兼类使用程度高，兼类现象涉及汉语中大部分词类，因而造成在汉语文本中词类歧义排除的任务量巨大。

2. 研究者主观原因造成的困难。语言学界在词性划分的目的、标准等问题上还存在分歧。目前还没有一个统的被广泛认可汉语词类划分标准，词类划分的粒度和标记符号都不统一。词类划分标准和标记符号集的差异，以及分词规范的含混性，给中文信息处理带来了极大的困难。

解决方法以及其思想：
可以利用基于统计模型的词性标注方法，如隐马尔可夫，CRF等方法。其基本思想是：给定带有各自标注的词的序列，我们可以确定下一个词最可能的词性。



## 第三条

请给出隐马尔可夫模型HMM的五元组定义，并阐明定义中每个符号的含义，写出隐马尔可夫模型解码问题的维特比算法的公式细节。

在隐马尔科夫模型中，包含隐状态 和 观察状态，隐状态 $i_i$ 对于观察者而言是不可见的，而观察状态 $o_i$ 对于观察者而言是可见的。隐状态间存在转移概率，隐状态 $i_i$到对应的观察状态 $o_i$ 间存在输出概率。

- 隐藏序列：隐状态 $i_i$ 对于观察者而言是不可见的
- 观测序列：$o_i$ 对于观察者而言是可见的
- 初始状态矩阵：每个标签的概率矩阵
- 发射状态矩阵：一个字变成每个标签的概率 $ B=\left[b_{i j}\right]_{N \times M} $（$N$为隐藏状态集元素个数，M为观测集元素个数），其中$b_{i j}=P\left(o_{t} | i_{t}\right)$，$(o_{t}$为第i个观测节点 ，$i_t$ 为第i个隐状态节点,即所谓的观测概率（发射概率）；
- 状态转移级证：标签到每个标签的概率 $A=\left[a_{i j}\right]_{N \times N}$ （N 表示隐藏状态集元素的个数），其中  $a_{i j}=P\left(i_{t+1} | i_{t}\right)$，$i_t$  即第i个隐状态节点，即所谓的状态转移；


#### 维特比（Viterbi）算法

维特比算法则通过动态规划求概率最大的路径（最优路径），这时每一条路径即对应着一个状态序列。维特比算法从时刻t=1开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直到得到时刻t=T状态为i的各条路径的最大概率，时刻t=T的最大概率记为最优路径的概率P，最优路径的终结点也同时得到，之后，从终结点开始，由后向前逐步求得结点。

1. 输入模型λ和观测O；

2. 初始化:

$$
\begin{array}{c}
\delta_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N \\
\psi_{1}(i)=0, \quad i=1,2, \cdots, N
\end{array}
$$

3. 递推，对t=2,3,···,T

$$
\begin{array}{c}
\delta_{t}(i)=\max _{1 \leq j \leq N}\left[\delta_{t-1}(j) a_{j i}\right] b_{i}\left(o_{t}\right), \quad i=1,2, \cdots, N \\
\psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], \quad i=1,2, \cdots, N
\end{array}
$$

4. 终止

$$
\begin{array}{c}
P^{*}=\max _{1 \leq i \leqslant N} \delta_{T}(i) \\
i_{T}^{*}=\arg \max _{1 \leq i \leqslant N}\left[\delta_{T}(i)\right]
\end{array}
$$

5. 最终路径回溯，对t=T-l，T-2，…，1，并得到最终路径

$$
\begin{array}{c}
i_{t}^{*}=\psi_{t+1}\left(i_{t+1}^{*}\right) \\
I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right)
\end{array}
$$



## 第四条

请写出分词任务的评价指标公式，给出正确情况和错误情况的详细说明，并举例

评价指标公式有：准确率（accuracy），精确率（presision），召回率（recall），F_score

公式如下：

$$
\begin{aligned}
ACC &= \frac {TP + TN}{TP + FP + FN + TN} \\
Precision &= \frac {TP}{TP + FP} \\
Recall &= \frac {TP}{TP + FN} \\
F_{score} &= (1+ \beta^2) \frac {Precision \cdot Recall}{ \beta^2 \cdot Precision + Recall } \\
\end{aligned}
$$

True Positive（TP）：表示将正样本预测为正样本，即预测正确；
False Positive（FP）：表示将负样本预测为正样本，即预测错误；
False Negative（FN）：表示将正样本预测为负样本，即预测错误；
True Negative（TN）：表示将负样本预测为负样本，即预测正确；

#### 举例

数据集中有25张猫的图片，75张狗的图片；现有如下情况：将猫预测为猫的数量为20，将猫预测为狗的数量为6，同理有18表示将狗预测成猫的数量，57表示将狗预测为狗的数量。

对于猫来说：
$$
\begin{aligned}
\text { Precision } &=\frac{20}{20+18}=0.53 \\
\text { Recall } &=\frac{20}{20+5}=0.8 \\
F_{1} &=\frac{2 \times 0.53 \times 0.8}{0.53+0.8}=0.63
\end{aligned}
$$


对于狗来说：
$$
\begin{aligned}
\text { Precision } &=\frac{57}{57+5}=0.92 \\
\text { Recall } &=\frac{57}{57+18}=0.76 \\
F_{1} &=\frac{2 \times 0.92 \times 0.76}{0.92+0.76}=0.83
\end{aligned}
$$



## 第五条

给出基于卷积神经网络的文本语义编码网络的简单层级描述，并给出详细计算公式（20分）

卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。因此文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。

#### 简单层级描述

第一层为输入层。首先进行embedding，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。

第二层为卷积层。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了。

最后接一层全连接的 softmax 层，输出每个类别的概率。

#### 详细的公式计算

通过一个filter作用一个词窗口提取可以提取一个特征  $c_{i} $, 如下:

$$
c_{i}=f(W \cdot X_{i: i+h-1} +b)
$$

其中,  $b \in R $ 是bias值,  f  为激活函数如Relu等。
卷积操作：通过一个filter在整个句子上从句首到句尾扫描一遍，提取每个词窗口的特征, 可以得到一个特征图(feature map)  $c \in R^{n-h+1} $, 表示如下 (这里默认不对句子进行padding):
$$
c=\left[c_{1}, c_{2}, \ldots, c n-h+1\right]
$$

池化操作：对一个filter提取到的feature map进行max pooling, 得到  \hat{c} \in \Re  即:

$$
\hat{c}=\max (c)
$$

若有  m  个filter, 则通过一层卷积、一层池化后可以得到一个长度为  m  的向量  $z \in R^{m} $ :

$$
z=\left[\hat{c}_{1}, \hat{c}_{2}, \ldots, \hat{c}_{m}\right]
$$

最后, 将向量  z  输入到全连接层, 得到最终的特征提取向量  y  (这里的  W  为全连接层的权重, 注意与filter进行区分):

$$
y=W \cdot z+b
$$



## 第六条

请写出长短时记忆网络LSTM中每个单元的计算公式，并结合公式说明LSTM是如何缓解梯度消失问题的？给出符号意义的说明。（10分）

#### 公式计算
输入门  :
$$
i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right) 
$$

遗忘门:
$$
f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right) 
 \tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right) 
$$

输出门:
$$
o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right) 
$$

符号说明（隐藏层长短记忆的公式）：

$$
长记忆:  C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t} 
短记忆:  h_{t}=o_{t} * \tanh \left(C_{t}\right) 
$$

#### LSTM是如何缓解梯度消失问题

- 引用门控机制
  - 遗忘门：控制继续保存长期状态c；
  - 输入门：控制把即时状态输入到长期状态c；
  - 输出门：控制是否把长期状态c作为当前的LSTM的输出；
- 原理：门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。



## 第七条

分别从问题假设和模型本质等不同角度，说明隐马尔可夫模型HMM和条件随机场CRF的区别，给出必要的公式。（15分）

- 相同点：HMM、CRF 都常用于 序列标注任务；
- 不同点：
  - HMM 因其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择的问题；
  - CRF 很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。
	- CRF模型属于判别式模型，在 序列标注任务上，效果优于生成式模型；
	- HMM 提出齐次马尔可夫性假设和观测独立性假设，这两个假设过强，而 CRF 只需要满足 局部马尔可夫性就好，通过降低假设的方式，提升模型效果；



## 第八条

因为这份问卷有很多公式，所以用了公式编辑的网站：
https://www.latexlive.com/home##

博客：
https://blog.csdn.net/jyfu2_12/article/details/79207643
https://blog.csdn.net/zgcr654321/article/details/92639420
https://www.cnblogs.com/pinard/p/6945257.html
https://zhuanlan.zhihu.com/p/100552669
https://zhuanlan.zhihu.com/p/77634533
https://www.zhihu.com/question/53458773
https://github.com/km1994/NLP-Interview-Notes/



## 第九条

关于第三章、第四章和第五章知识点，请写下你仍然存在的问题。

Q1:
上课有很多公式详细的推导都忘记了，不知道有没有补充材料

Q2:
最大熵模型没太听懂他的优化目标

Q3：
统计机器学习的公式推导很多地方没听懂，感觉好难，能有上课的回放吗


## 补充
